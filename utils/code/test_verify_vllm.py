#!/usr/bin/env python3
"""
éªŒè¯ vLLM æ˜¯å¦æ­£ç¡®å®‰è£…å’Œå¯ç”¨
ç”¨äºæ£€æŸ¥ vLLM æ˜¯å¦å¯ä»¥æ­£å¸¸å¯¼å…¥å’Œä½¿ç”¨
"""

import sys
import subprocess

def check_vllm_installation():
    """æ£€æŸ¥ vLLM å®‰è£…"""
    print("=" * 60)
    print("æµ‹è¯• 1: vLLM å®‰è£…æ£€æŸ¥")
    print("=" * 60)
    
    try:
        import vllm
        version = getattr(vllm, '__version__', 'æœªçŸ¥')
        print(f"âœ“ vLLM ç‰ˆæœ¬: {version}")
        return True, version
    except ImportError as e:
        print(f"âœ— vLLM æœªå®‰è£…")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        return False, None
    except Exception as e:
        print(f"âœ— vLLM å¯¼å…¥å¤±è´¥")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        return False, None

def check_pytorch():
    """æ£€æŸ¥ PyTorch ç¯å¢ƒ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: PyTorch ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    try:
        import torch
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"  CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"  GPU æ•°é‡: {torch.cuda.device_count()}")
            
            # è·å– CUDA é©±åŠ¨ç‰ˆæœ¬
            try:
                result = subprocess.run(
                    ['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0 and result.stdout.strip():
                    driver_version = result.stdout.strip().split('\n')[0]
                    print(f"  CUDA é©±åŠ¨ç‰ˆæœ¬: {driver_version}")
            except:
                pass
        
        return True
    except ImportError as e:
        print(f"âœ— PyTorch æœªå®‰è£…: {e}")
        return False
    except Exception as e:
        print(f"âœ— PyTorch æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_vllm_critical_modules():
    """æµ‹è¯• vLLM å…³é”®æ¨¡å—ï¼ˆè¿™æ˜¯ä¼šå‡ºé”™çš„åœ°æ–¹ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: vLLM å…³é”®æ¨¡å—å¯¼å…¥ï¼ˆå…³é”®æµ‹è¯•ï¼‰")
    print("=" * 60)
    
    try:
        # æµ‹è¯• vllm.platforms
        from vllm.platforms import current_platform
        print("âœ“ vllm.platforms å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯• vllm._Cï¼ˆC++ æ‰©å±•ï¼Œæœ€å®¹æ˜“å‡ºé”™ï¼‰
        import vllm._C
        print("âœ“ vllm._C å¯¼å…¥æˆåŠŸ")
        
        print("\nâœ“ vLLM å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
        return True
    except ImportError as e:
        print(f"âœ— vLLM å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("\nè¿™æ˜¯å¯¼è‡´è®­ç»ƒå¤±è´¥çš„å…³é”®é”™è¯¯ï¼")
        print("\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
        print("1. é‡æ–°å®‰è£… vLLM:")
        print("   pip uninstall vllm -y")
        print("   pip install vllm==0.11.0 --no-deps")
        print("\n2. æˆ–è€…å®‰è£…å®Œæ•´ç‰ˆæœ¬:")
        print("   pip install vllm==0.11.0")
        print("\n3. æ£€æŸ¥ CUDA å’Œ PyTorch ç‰ˆæœ¬å…¼å®¹æ€§")
        return False
    except Exception as e:
        error_msg = str(e)
        print(f"âœ— vLLM å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        
        # è¯Šæ–­å¸¸è§é”™è¯¯
        if 'undefined symbol' in error_msg:
            print("\n" + "-" * 60)
            print("ç¬¦å·é”™è¯¯è¯Šæ–­:")
            print("-" * 60)
            print("æ£€æµ‹åˆ°ç¬¦å·æœªå®šä¹‰é”™è¯¯ï¼Œå¯èƒ½çš„åŸå› ï¼š")
            print("1. vLLM ä¸å½“å‰ PyTorch ç‰ˆæœ¬ä¸å…¼å®¹")
            print("2. CUDA åº“è·¯å¾„é…ç½®ä¸æ­£ç¡®")
            print("3. vLLM éœ€è¦é‡æ–°ç¼–è¯‘")
            print("\nå»ºè®®ï¼š")
            print("1. è®¾ç½® CUDA åº“è·¯å¾„:")
            print("   export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:$LD_LIBRARY_PATH")
            print("   export LD_PRELOAD=/usr/local/nvidia/lib64/libcuda.so")
            print("\n2. é‡æ–°å®‰è£… vLLM")
        
        return False

def test_vllm_basic_functionality():
    """æµ‹è¯• vLLM åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: vLLM åŸºæœ¬åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        import vllm
        from vllm import LLM
        
        # æ£€æŸ¥ LLM ç±»æ˜¯å¦å¯ç”¨
        print("âœ“ vLLM.LLM ç±»å¯ç”¨")
        
        # æ£€æŸ¥å…¶ä»–å…³é”®ç»„ä»¶
        try:
            from vllm.engine.arg_utils import AsyncEngineArgs
            print("âœ“ vLLM å¼•æ“å‚æ•°ç±»å¯ç”¨")
        except:
            pass
        
        try:
            from vllm.worker.worker import Worker
            print("âœ“ vLLM Worker ç±»å¯ç”¨")
        except:
            pass
        
        print("âœ“ vLLM åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        return True
    except Exception as e:
        print(f"âœ— vLLM åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("vLLM å®‰è£…å’Œå¯ç”¨æ€§æµ‹è¯•")
    print("=" * 60 + "\n")
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    vllm_installed, vllm_version = check_vllm_installation()
    if not vllm_installed:
        print("\n" + "=" * 60)
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 60)
        print("âœ— vLLM æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… vLLM")
        print("\nå®‰è£…å‘½ä»¤:")
        print("  pip install vllm==0.11.0")
        return 1
    
    results.append(("vLLM å®‰è£…", vllm_installed))
    results.append(("PyTorch ç¯å¢ƒ", check_pytorch()))
    results.append(("vLLM å…³é”®æ¨¡å—", test_vllm_critical_modules()))
    
    # åªåœ¨å…³é”®æ¨¡å—é€šè¿‡æ—¶æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    if results[-1][1]:  # å¦‚æœå…³é”®æ¨¡å—æµ‹è¯•é€šè¿‡
        results.append(("vLLM åŸºæœ¬åŠŸèƒ½", test_vllm_basic_functionality()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼vLLM å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
        
        # å¦‚æœå…³é”®æ¨¡å—å¤±è´¥ï¼Œæä¾›å¿«é€Ÿä¿®å¤å»ºè®®
        if not results[-1][1] if results else False:
            print("\n" + "=" * 60)
            print("å¿«é€Ÿä¿®å¤å»ºè®®")
            print("=" * 60)
            print("å¦‚æœ vLLM å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå¯ä»¥å°è¯•ï¼š")
            print("\n1. é‡æ–°å®‰è£… vLLM:")
            print("   pip uninstall vllm -y")
            print("   pip install vllm==0.11.0 --no-deps")
            print("\n2. è®¾ç½® CUDA åº“è·¯å¾„ï¼ˆå¦‚æœé‡åˆ°ç¬¦å·é”™è¯¯ï¼‰:")
            print("   export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:$LD_LIBRARY_PATH")
            print("   export LD_PRELOAD=/usr/local/nvidia/lib64/libcuda.so")
            print("\n3. æ£€æŸ¥ PyTorch å’Œ CUDA ç‰ˆæœ¬å…¼å®¹æ€§")
        
        return 1

if __name__ == "__main__":
    sys.exit(main())

