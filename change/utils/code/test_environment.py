#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„ç¯å¢ƒæµ‹è¯•è„šæœ¬
ç”¨äºåœ¨å®¹å™¨ä¸­æµ‹è¯•æ·±åº¦å­¦ä¹ ç¯å¢ƒæ˜¯å¦æ­£å¸¸
åŒ…æ‹¬ï¼šcuDNNã€Tritonã€APEXã€Flash Attentionã€vLLM çš„æµ‹è¯•
"""

import sys
import os
import subprocess
import glob
import shutil
import importlib
from pathlib import Path

# ============================================================================
# é€šç”¨å·¥å…·å‡½æ•°
# ============================================================================

def print_section(title, level=1):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    if level == 1:
        print("\n" + "=" * 80)
        print(title)
        print("=" * 80)
    elif level == 2:
        print("\n" + "-" * 80)
        print(title)
        print("-" * 80)
    else:
        print(f"\n{title}")

def get_cuda_driver_version():
    """è·å– CUDA é©±åŠ¨ç‰ˆæœ¬"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0 and result.stdout.strip():
            return result.stdout.strip().split('\n')[0]
    except:
        pass
    
    try:
        result = subprocess.run(
            ['nvidia-smi'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            for line in result.stdout.split('\n'):
                if 'Driver Version:' in line:
                    return line.split('Driver Version:')[1].strip().split()[0]
    except:
        pass
    
    return None

def check_pytorch_basic():
    """æ£€æŸ¥ PyTorch åŸºæœ¬ä¿¡æ¯"""
    try:
        import torch
        info = {
            'version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': None,
            'gpu_count': 0,
            'driver_version': None
        }
        
        if torch.cuda.is_available():
            info['cuda_version'] = torch.version.cuda
            info['gpu_count'] = torch.cuda.device_count()
            info['driver_version'] = get_cuda_driver_version()
        
        return True, info
    except ImportError:
        return False, None
    except Exception as e:
        return False, {'error': str(e)}

# ============================================================================
# cuDNN è¯Šæ–­æµ‹è¯•
# ============================================================================

def test_cudnn():
    """cuDNN è¯Šæ–­æµ‹è¯•"""
    print_section("æµ‹è¯•æ¨¡å— 1: cuDNN è¯Šæ–­", 1)
    
    results = {}
    
    # æ£€æŸ¥ LD_LIBRARY_PATH
    print_section("æ£€æŸ¥ 1: LD_LIBRARY_PATH ç¯å¢ƒå˜é‡", 2)
    ld_path = os.environ.get('LD_LIBRARY_PATH', '')
    print(f"å½“å‰ LD_LIBRARY_PATH: {ld_path if ld_path else '(æœªè®¾ç½®)'}")
    
    paths = ld_path.split(':') if ld_path else []
    cudnn_paths = []
    
    for path in paths:
        if path and os.path.exists(path):
            cudnn_libs = glob.glob(os.path.join(path, '*cudnn*'))
            if cudnn_libs:
                cudnn_paths.append(path)
                print(f"  âœ“ æ‰¾åˆ° cuDNN åº“è·¯å¾„: {path}")
    
    if not cudnn_paths:
        print("  âš  åœ¨ LD_LIBRARY_PATH ä¸­æœªæ‰¾åˆ° cuDNN åº“")
    
    # æŸ¥æ‰¾ cuDNN åº“æ–‡ä»¶
    print_section("æ£€æŸ¥ 2: æŸ¥æ‰¾ cuDNN åº“æ–‡ä»¶", 2)
    common_paths = [
        '/usr/local/cuda/lib64',
        '/usr/local/cuda/lib',
        '/usr/lib/x86_64-linux-gnu',
        '/usr/local/nvidia/lib64',
        '/usr/local/nvidia/lib',
        '/opt/conda/lib',
        '/opt/conda/envs/*/lib',
    ]
    
    if ld_path:
        common_paths.extend(ld_path.split(':'))
    
    found_libs = {}
    search_patterns = ['libcudnn*.so*', 'libcudnn*.so']
    
    for base_path in common_paths:
        if '*' in base_path:
            expanded = glob.glob(base_path)
            search_paths = expanded if expanded else []
        else:
            search_paths = [base_path] if os.path.exists(base_path) else []
        
        for search_path in search_paths:
            if not os.path.isdir(search_path):
                continue
                
            for pattern in search_patterns:
                libs = glob.glob(os.path.join(search_path, pattern))
                if libs:
                    if search_path not in found_libs:
                        found_libs[search_path] = []
                    found_libs[search_path].extend(libs)
    
    if found_libs:
        print("âœ“ æ‰¾åˆ° cuDNN åº“æ–‡ä»¶:")
        for path, libs in found_libs.items():
            print(f"  è·¯å¾„: {path}")
            for lib in sorted(set(libs))[:3]:
                print(f"    - {os.path.basename(lib)}")
    else:
        print("âœ— æœªæ‰¾åˆ° cuDNN åº“æ–‡ä»¶")
    
    # æ£€æŸ¥ PyTorch cuDNN
    print_section("æ£€æŸ¥ 3: PyTorch cuDNN é…ç½®", 2)
    pytorch_ok = None
    cudnn_version = None
    
    try:
        import torch
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"  CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            
            try:
                cudnn_enabled = torch.backends.cudnn.enabled
                cudnn_version = torch.backends.cudnn.version()
                print(f"  cuDNN å·²å¯ç”¨: {cudnn_enabled}")
                print(f"  cuDNN ç‰ˆæœ¬: {cudnn_version}")
                
                # æµ‹è¯• cuDNN åŠŸèƒ½
                try:
                    device = torch.device('cuda:0')
                    x = torch.randn(1, 3, 224, 224, device=device)
                    conv = torch.nn.Conv2d(3, 64, 3, padding=1).to(device)
                    with torch.backends.cudnn.flags(enabled=True, benchmark=False, deterministic=False):
                        y = conv(x)
                    print("  âœ“ cuDNN åŠŸèƒ½æµ‹è¯•é€šè¿‡")
                    pytorch_ok = True
                except Exception as e:
                    print(f"  âœ— cuDNN åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
                    pytorch_ok = False
                    
            except Exception as e:
                print(f"  âœ— æ— æ³•è·å– cuDNN ä¿¡æ¯: {e}")
                pytorch_ok = False
        else:
            print("  âš  CUDA ä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯• cuDNN")
            
    except ImportError:
        print("âœ— PyTorch æœªå®‰è£…")
    except Exception as e:
        print(f"âœ— æ£€æŸ¥ PyTorch æ—¶å‡ºé”™: {e}")
    
    results['ld_paths'] = paths
    results['cudnn_libs'] = found_libs
    results['pytorch_ok'] = pytorch_ok
    results['cudnn_version'] = cudnn_version
    
    return results

# ============================================================================
# Triton å…¼å®¹æ€§æµ‹è¯•
# ============================================================================

def test_triton():
    """Triton å…¼å®¹æ€§æµ‹è¯•"""
    print_section("æµ‹è¯•æ¨¡å— 2: Triton å…¼å®¹æ€§", 1)
    
    results = {}
    
    # åŸºæœ¬å¯¼å…¥æµ‹è¯•
    print_section("æµ‹è¯• 1: åŸºæœ¬å¯¼å…¥", 2)
    try:
        import torch
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"  GPU æ•°é‡: {torch.cuda.device_count()}")
            driver_version = get_cuda_driver_version()
            if driver_version:
                print(f"  CUDA é©±åŠ¨ç‰ˆæœ¬: {driver_version}")
        results['torch_ok'] = True
    except ImportError as e:
        print(f"âœ— PyTorch å¯¼å…¥å¤±è´¥: {e}")
        results['torch_ok'] = False
        return results
    
    try:
        import triton
        print(f"âœ“ Triton ç‰ˆæœ¬: {triton.__version__}")
        results['triton_installed'] = True
    except ImportError as e:
        print(f"âœ— Triton å¯¼å…¥å¤±è´¥: {e}")
        results['triton_installed'] = False
        return results
    
    # æ£€æŸ¥ CUDA åº“è·¯å¾„
    print_section("CUDA åº“è·¯å¾„æ£€æŸ¥", 2)
    ld_path = os.environ.get('LD_LIBRARY_PATH', '')
    ld_preload = os.environ.get('LD_PRELOAD', '')
    print(f"LD_LIBRARY_PATH: {ld_path if ld_path else '(æœªè®¾ç½®)'}")
    print(f"LD_PRELOAD: {ld_preload if ld_preload else '(æœªè®¾ç½®)'}")
    
    common_paths = [
        '/usr/local/nvidia/lib64',
        '/usr/local/cuda/lib64',
        '/usr/lib/x86_64-linux-gnu',
    ]
    
    libcuda_found = False
    for path in common_paths:
        libcuda_path = os.path.join(path, 'libcuda.so')
        if os.path.exists(libcuda_path):
            print(f"âœ“ æ‰¾åˆ° libcuda.so: {libcuda_path}")
            libcuda_found = True
            break
    
    if not libcuda_found:
        print("âš  æœªæ‰¾åˆ° libcuda.soï¼Œè¿™å¯èƒ½å¯¼è‡´ cuModuleGetFunction é”™è¯¯")
    
    results['libcuda_found'] = libcuda_found
    
    # triton_key å¯¼å…¥æµ‹è¯•
    print_section("æµ‹è¯• 2: triton_key å¯¼å…¥ï¼ˆå…³é”®æµ‹è¯•ï¼‰", 2)
    try:
        from triton.compiler.compiler import triton_key
        print("âœ“ triton_key å¯¼å…¥æˆåŠŸï¼")
        print(f"  triton_key ç±»å‹: {type(triton_key)}")
        results['triton_key_ok'] = True
    except ImportError as e:
        print(f"âœ— triton_key å¯¼å…¥å¤±è´¥: {e}")
        print("\nè¿™æ˜¯å¯¼è‡´è®­ç»ƒå¤±è´¥çš„å…³é”®é”™è¯¯ï¼")
        results['triton_key_ok'] = False
    except Exception as e:
        print(f"âœ— å…¶ä»–é”™è¯¯: {e}")
        results['triton_key_ok'] = False
    
    # PyTorch Inductor æµ‹è¯•
    print_section("æµ‹è¯• 3: PyTorch Inductor ç¼“å­˜ç³»ç»Ÿ", 2)
    try:
        from torch._inductor.codecache import CacheBase
        system_info = CacheBase.get_system()
        print("âœ“ PyTorch Inductor ç¼“å­˜ç³»ç»Ÿæ­£å¸¸")
        print(f"  ç³»ç»Ÿä¿¡æ¯é”®: {list(system_info.keys())[:3]}...")
        results['inductor_ok'] = True
    except Exception as e:
        print(f"âœ— PyTorch Inductor æµ‹è¯•å¤±è´¥: {e}")
        results['inductor_ok'] = False
    
    # torch.compile æµ‹è¯•
    print_section("æµ‹è¯• 4: torch.compile åŸºæœ¬åŠŸèƒ½", 2)
    if torch.cuda.is_available():
        try:
            @torch.compile
            def simple_add(x, y):
                return x + y
            
            x = torch.randn(10, 10, device='cuda')
            y = torch.randn(10, 10, device='cuda')
            result = simple_add(x, y)
            print("âœ“ torch.compile æµ‹è¯•é€šè¿‡")
            print(f"  ç»“æœå½¢çŠ¶: {result.shape}")
            results['compile_ok'] = True
        except Exception as e:
            error_msg = str(e)
            print(f"âœ— torch.compile æµ‹è¯•å¤±è´¥: {e}")
            if 'undefined symbol' in error_msg or 'cuModuleGetFunction' in error_msg:
                print("  è¿™æ˜¯ CUDA é©±åŠ¨åº“è·¯å¾„é…ç½®é—®é¢˜")
            results['compile_ok'] = False
    else:
        print("âš  è·³è¿‡ï¼šæœªæ£€æµ‹åˆ° GPU")
        results['compile_ok'] = None
    
    return results

# ============================================================================
# APEX éªŒè¯æµ‹è¯•
# ============================================================================

def diagnose_symbol_error(error_msg):
    """è¯Šæ–­ç¬¦å·æœªå®šä¹‰é”™è¯¯"""
    if 'undefined symbol' in str(error_msg):
        print("\n" + "-" * 60)
        print("ç¬¦å·é”™è¯¯è¯Šæ–­:")
        print("-" * 60)
        symbol = str(error_msg)
        if 'c10' in symbol or 'SetDevice' in symbol or '_ZN3c10' in symbol:
            print("âš  æ£€æµ‹åˆ° PyTorch ç¬¦å·æœªå®šä¹‰é”™è¯¯")
            print("  è¿™é€šå¸¸è¡¨ç¤º APEX ä¸å½“å‰ PyTorch ç‰ˆæœ¬ä¸å…¼å®¹")

def check_module(module_name, description):
    """æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ä»¥å¯¼å…¥"""
    try:
        mod = importlib.import_module(module_name)
        print(f"âœ“ {description}: {module_name} - å·²å®‰è£…")
        return True
    except ImportError as e:
        print(f"âœ— {description}: {module_name} - æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        diagnose_symbol_error(e)
        return False
    except Exception as e:
        print(f"âœ— {description}: {module_name} - å¯¼å…¥æ—¶å‡ºé”™")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        diagnose_symbol_error(e)
        return False

def test_apex():
    """APEX éªŒè¯æµ‹è¯•"""
    print_section("æµ‹è¯•æ¨¡å— 3: APEX éªŒè¯", 1)
    
    results = {}
    
    # æ£€æŸ¥ PyTorch
    print_section("ç¯å¢ƒä¿¡æ¯", 2)
    pytorch_ok, pytorch_info = check_pytorch_basic()
    if pytorch_ok:
        print(f"  PyTorch ç‰ˆæœ¬: {pytorch_info['version']}")
        if pytorch_info['cuda_available']:
            print(f"  CUDA ç‰ˆæœ¬: {pytorch_info['cuda_version']}")
    else:
        print("  âš  PyTorch æœªå®‰è£…")
    
    # æ£€æŸ¥åŸºç¡€æ¨¡å—
    print_section("æ£€æŸ¥åŸºç¡€ APEX æ¨¡å—", 2)
    apex_basic = check_module("apex", "APEX åŸºç¡€åŒ…")
    if apex_basic:
        try:
            import apex
            print(f"  APEX ç‰ˆæœ¬: {getattr(apex, '__version__', 'æœªçŸ¥')}")
        except:
            pass
    results['apex_basic'] = apex_basic
    
    if not apex_basic:
        results['status'] = 'not_installed'
        return results
    
    # æ£€æŸ¥ C++ æ‰©å±•
    print_section("æ£€æŸ¥ C++ æ‰©å±• (APEX_CPP_EXT)", 2)
    apex_c = check_module("apex_C", "APEX C++ æ‰©å±•")
    results['apex_c'] = apex_c
    
    # æ£€æŸ¥ CUDA æ‰©å±•
    print_section("æ£€æŸ¥ CUDA æ‰©å±• (APEX_CUDA_EXT)", 2)
    cuda_extensions = [
        ("amp_C", "AMP CUDA æ‰©å±•"),
        ("syncbn", "SyncBatchNorm CUDA æ‰©å±•"),
        ("fused_layer_norm_cuda", "Fused LayerNorm CUDA æ‰©å±•"),
        ("mlp_cuda", "MLP CUDA æ‰©å±•"),
        ("fused_weight_gradient_mlp_cuda", "Fused Weight Gradient MLP CUDA æ‰©å±• (gradient_accumulation_fusion éœ€è¦)"),
        ("scaled_upper_triang_masked_softmax_cuda", "Scaled Upper Triangular Masked Softmax CUDA æ‰©å±•"),
        ("generic_scaled_masked_softmax_cuda", "Generic Scaled Masked Softmax CUDA æ‰©å±•"),
        ("scaled_masked_softmax_cuda", "Scaled Masked Softmax CUDA æ‰©å±•"),
    ]
    
    cuda_results = {}
    for module_name, description in cuda_extensions:
        result = check_module(module_name, description)
        cuda_results[module_name] = result
    
    results['cuda_extensions'] = cuda_results
    
    # æ£€æŸ¥å…³é”®æ¨¡å—
    print_section("å…³é”®æ¨¡å—æ£€æŸ¥ (gradient_accumulation_fusion å¿…éœ€)", 2)
    critical_module = "fused_weight_gradient_mlp_cuda"
    critical_ok = cuda_results.get(critical_module, False)
    if critical_ok:
        print(f"âœ“ {critical_module} å·²æ­£ç¡®å®‰è£…")
        print("  â†’ gradient_accumulation_fusion åŠŸèƒ½å¯ç”¨")
    else:
        print(f"âœ— {critical_module} æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥")
        print("  â†’ gradient_accumulation_fusion åŠŸèƒ½ä¸å¯ç”¨")
    
    results['critical_ok'] = critical_ok
    results['status'] = 'ok' if (apex_basic and critical_ok) else 'partial'
    
    return results

# ============================================================================
# Flash Attention éªŒè¯æµ‹è¯•
# ============================================================================

def test_flash_attn():
    """Flash Attention éªŒè¯æµ‹è¯•"""
    print_section("æµ‹è¯•æ¨¡å— 4: Flash Attention éªŒè¯", 1)
    
    results = {}
    
    # æ£€æŸ¥å®‰è£…
    print_section("æµ‹è¯• 1: Flash Attention å®‰è£…æ£€æŸ¥", 2)
    try:
        import flash_attn
        version = getattr(flash_attn, '__version__', 'æœªçŸ¥')
        print(f"âœ“ flash_attn ç‰ˆæœ¬: {version}")
        results['installed'] = True
        results['version'] = version
    except ImportError as e:
        print(f"âœ— flash_attn æœªå®‰è£…")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        results['installed'] = False
        return results
    except Exception as e:
        print(f"âœ— flash_attn å¯¼å…¥å¤±è´¥")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        results['installed'] = False
        return results
    
    # æ£€æŸ¥ PyTorch
    print_section("æµ‹è¯• 2: PyTorch ç¯å¢ƒæ£€æŸ¥", 2)
    pytorch_ok, pytorch_info = check_pytorch_basic()
    if pytorch_ok:
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {pytorch_info['version']}")
        print(f"  CUDA å¯ç”¨: {pytorch_info['cuda_available']}")
        if pytorch_info['cuda_available']:
            print(f"  CUDA ç‰ˆæœ¬: {pytorch_info['cuda_version']}")
            print(f"  GPU æ•°é‡: {pytorch_info['gpu_count']}")
            if pytorch_info['driver_version']:
                print(f"  CUDA é©±åŠ¨ç‰ˆæœ¬: {pytorch_info['driver_version']}")
    else:
        print("âœ— PyTorch æœªå®‰è£…")
        results['pytorch_ok'] = False
        return results
    
    # å…³é”®æ¨¡å—æµ‹è¯•
    print_section("æµ‹è¯• 3: Flash Attention å…³é”®æ¨¡å—å¯¼å…¥ï¼ˆå…³é”®æµ‹è¯•ï¼‰", 2)
    try:
        import flash_attn
        print("âœ“ flash_attn ä¸»æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        try:
            import flash_attn.flash_attn_interface
            print("âœ“ flash_attn.flash_attn_interface å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âš  flash_attn.flash_attn_interface å¯¼å…¥å¤±è´¥: {e}")
        
        try:
            from flash_attn import flash_attn_func
            print("âœ“ flash_attn_func å¯¼å…¥æˆåŠŸ")
            results['critical_modules_ok'] = True
        except ImportError as e:
            print(f"âœ— flash_attn_func å¯¼å…¥å¤±è´¥: {e}")
            print("\nè¿™æ˜¯å¯¼è‡´è®­ç»ƒå¤±è´¥çš„å…³é”®é”™è¯¯ï¼")
            results['critical_modules_ok'] = False
            return results
        
        try:
            from flash_attn import flash_attn_varlen_func
            print("âœ“ flash_attn_varlen_func å¯¼å…¥æˆåŠŸ")
        except ImportError:
            print("âš  flash_attn_varlen_func ä¸å¯ç”¨ï¼ˆå¯é€‰ï¼‰")
        
        print("\nâœ“ Flash Attention å…³é”®æ¨¡å—å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
        
    except ImportError as e:
        print(f"âœ— Flash Attention å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        results['critical_modules_ok'] = False
        return results
    except Exception as e:
        error_msg = str(e)
        print(f"âœ— Flash Attention å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        if 'undefined symbol' in error_msg:
            print("\næ£€æµ‹åˆ°ç¬¦å·æœªå®šä¹‰é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç‰ˆæœ¬ä¸å…¼å®¹æˆ– CUDA åº“è·¯å¾„é—®é¢˜")
        results['critical_modules_ok'] = False
        return results
    
    # åŠŸèƒ½æµ‹è¯•
    print_section("æµ‹è¯• 4: Flash Attention åŸºæœ¬åŠŸèƒ½æµ‹è¯•", 2)
    if pytorch_info['cuda_available']:
        try:
            import torch
            from flash_attn import flash_attn_func
            
            batch_size = 2
            seq_len = 128
            num_heads = 8
            head_dim = 64
            
            q = torch.randn(batch_size, seq_len, num_heads, head_dim, device='cuda', dtype=torch.float16)
            k = torch.randn(batch_size, seq_len, num_heads, head_dim, device='cuda', dtype=torch.float16)
            v = torch.randn(batch_size, seq_len, num_heads, head_dim, device='cuda', dtype=torch.float16)
            
            output = flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False)
            print(f"âœ“ flash_attn_func æ‰§è¡ŒæˆåŠŸ")
            print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
            results['functionality_ok'] = True
        except Exception as e:
            print(f"âœ— flash_attn_func æ‰§è¡Œå¤±è´¥: {e}")
            results['functionality_ok'] = False
    else:
        print("âš  è·³è¿‡åŠŸèƒ½æµ‹è¯•ï¼šæœªæ£€æµ‹åˆ° GPU")
        results['functionality_ok'] = None
    
    return results

# ============================================================================
# vLLM éªŒè¯æµ‹è¯•
# ============================================================================

def test_vllm():
    """vLLM éªŒè¯æµ‹è¯•"""
    print_section("æµ‹è¯•æ¨¡å— 5: vLLM éªŒè¯", 1)
    
    results = {}
    
    # æ£€æŸ¥å®‰è£…
    print_section("æµ‹è¯• 1: vLLM å®‰è£…æ£€æŸ¥", 2)
    try:
        import vllm
        version = getattr(vllm, '__version__', 'æœªçŸ¥')
        print(f"âœ“ vLLM ç‰ˆæœ¬: {version}")
        results['installed'] = True
        results['version'] = version
    except ImportError as e:
        print(f"âœ— vLLM æœªå®‰è£…")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        results['installed'] = False
        return results
    except Exception as e:
        print(f"âœ— vLLM å¯¼å…¥å¤±è´¥")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        results['installed'] = False
        return results
    
    # æ£€æŸ¥ PyTorch
    print_section("æµ‹è¯• 2: PyTorch ç¯å¢ƒæ£€æŸ¥", 2)
    pytorch_ok, pytorch_info = check_pytorch_basic()
    if pytorch_ok:
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {pytorch_info['version']}")
        print(f"  CUDA å¯ç”¨: {pytorch_info['cuda_available']}")
        if pytorch_info['cuda_available']:
            print(f"  CUDA ç‰ˆæœ¬: {pytorch_info['cuda_version']}")
            print(f"  GPU æ•°é‡: {pytorch_info['gpu_count']}")
            if pytorch_info['driver_version']:
                print(f"  CUDA é©±åŠ¨ç‰ˆæœ¬: {pytorch_info['driver_version']}")
    else:
        print("âœ— PyTorch æœªå®‰è£…")
        results['pytorch_ok'] = False
        return results
    
    # å…³é”®æ¨¡å—æµ‹è¯•
    print_section("æµ‹è¯• 3: vLLM å…³é”®æ¨¡å—å¯¼å…¥ï¼ˆå…³é”®æµ‹è¯•ï¼‰", 2)
    try:
        from vllm.platforms import current_platform
        print("âœ“ vllm.platforms å¯¼å…¥æˆåŠŸ")
        
        import vllm._C
        print("âœ“ vllm._C å¯¼å…¥æˆåŠŸ")
        
        print("\nâœ“ vLLM å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
        results['critical_modules_ok'] = True
    except ImportError as e:
        print(f"âœ— vLLM å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("\nè¿™æ˜¯å¯¼è‡´è®­ç»ƒå¤±è´¥çš„å…³é”®é”™è¯¯ï¼")
        results['critical_modules_ok'] = False
        return results
    except Exception as e:
        error_msg = str(e)
        print(f"âœ— vLLM å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        if 'undefined symbol' in error_msg:
            print("\næ£€æµ‹åˆ°ç¬¦å·æœªå®šä¹‰é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç‰ˆæœ¬ä¸å…¼å®¹æˆ– CUDA åº“è·¯å¾„é—®é¢˜")
        results['critical_modules_ok'] = False
        return results
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    print_section("æµ‹è¯• 4: vLLM åŸºæœ¬åŠŸèƒ½æµ‹è¯•", 2)
    try:
        from vllm import LLM
        print("âœ“ vLLM.LLM ç±»å¯ç”¨")
        
        try:
            from vllm.engine.arg_utils import AsyncEngineArgs
            print("âœ“ vLLM å¼•æ“å‚æ•°ç±»å¯ç”¨")
        except:
            pass
        
        results['functionality_ok'] = True
    except Exception as e:
        print(f"âœ— vLLM åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        results['functionality_ok'] = False
    
    return results

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 80)
    print("æ·±åº¦å­¦ä¹ ç¯å¢ƒç»Ÿä¸€æµ‹è¯•è„šæœ¬")
    print("=" * 80)
    print(f"Python ç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    
    all_results = {}
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    try:
        all_results['cudnn'] = test_cudnn()
    except Exception as e:
        print(f"\nâœ— cuDNN æµ‹è¯•å‡ºé”™: {e}")
        all_results['cudnn'] = {'error': str(e)}
    
    try:
        all_results['triton'] = test_triton()
    except Exception as e:
        print(f"\nâœ— Triton æµ‹è¯•å‡ºé”™: {e}")
        all_results['triton'] = {'error': str(e)}
    
    try:
        all_results['apex'] = test_apex()
    except Exception as e:
        print(f"\nâœ— APEX æµ‹è¯•å‡ºé”™: {e}")
        all_results['apex'] = {'error': str(e)}
    
    try:
        all_results['flash_attn'] = test_flash_attn()
    except Exception as e:
        print(f"\nâœ— Flash Attention æµ‹è¯•å‡ºé”™: {e}")
        all_results['flash_attn'] = {'error': str(e)}
    
    try:
        all_results['vllm'] = test_vllm()
    except Exception as e:
        print(f"\nâœ— vLLM æµ‹è¯•å‡ºé”™: {e}")
        all_results['vllm'] = {'error': str(e)}
    
    # æ±‡æ€»ç»“æœ
    print_section("æµ‹è¯•ç»“æœæ±‡æ€»", 1)
    
    summary = []
    
    # cuDNN
    cudnn_result = all_results.get('cudnn', {})
    if 'error' in cudnn_result:
        summary.append(("cuDNN", "é”™è¯¯", "æµ‹è¯•è¿‡ç¨‹å‡ºé”™"))
    elif cudnn_result.get('pytorch_ok') is True:
        summary.append(("cuDNN", "âœ“ é€šè¿‡", "cuDNN åŠŸèƒ½æ­£å¸¸"))
    elif cudnn_result.get('pytorch_ok') is False:
        summary.append(("cuDNN", "âœ— å¤±è´¥", "cuDNN åŠŸèƒ½æµ‹è¯•å¤±è´¥"))
    else:
        summary.append(("cuDNN", "âš  è·³è¿‡", "CUDA ä¸å¯ç”¨æˆ–æœªå®‰è£…"))
    
    # Triton
    triton_result = all_results.get('triton', {})
    if 'error' in triton_result:
        summary.append(("Triton", "é”™è¯¯", "æµ‹è¯•è¿‡ç¨‹å‡ºé”™"))
    elif not triton_result.get('triton_installed', False):
        summary.append(("Triton", "âš  æœªå®‰è£…", "Triton æœªå®‰è£…"))
    elif triton_result.get('triton_key_ok', False) and triton_result.get('inductor_ok', False):
        summary.append(("Triton", "âœ“ é€šè¿‡", "Triton å…¼å®¹æ€§æ­£å¸¸"))
    elif triton_result.get('triton_key_ok', False):
        summary.append(("Triton", "âš  éƒ¨åˆ†é€šè¿‡", "triton_key æ­£å¸¸ï¼Œä½† Inductor æœ‰é—®é¢˜"))
    else:
        summary.append(("Triton", "âœ— å¤±è´¥", "triton_key å¯¼å…¥å¤±è´¥"))
    
    # APEX
    apex_result = all_results.get('apex', {})
    if 'error' in apex_result:
        summary.append(("APEX", "é”™è¯¯", "æµ‹è¯•è¿‡ç¨‹å‡ºé”™"))
    elif not apex_result.get('apex_basic', False):
        summary.append(("APEX", "âš  æœªå®‰è£…", "APEX æœªå®‰è£…"))
    elif apex_result.get('critical_ok', False):
        summary.append(("APEX", "âœ“ é€šè¿‡", "APEX å…³é”®æ¨¡å—æ­£å¸¸"))
    else:
        summary.append(("APEX", "âœ— å¤±è´¥", "APEX å…³é”®æ¨¡å—ç¼ºå¤±"))
    
    # Flash Attention
    flash_attn_result = all_results.get('flash_attn', {})
    if 'error' in flash_attn_result:
        summary.append(("Flash Attention", "é”™è¯¯", "æµ‹è¯•è¿‡ç¨‹å‡ºé”™"))
    elif not flash_attn_result.get('installed', False):
        summary.append(("Flash Attention", "âš  æœªå®‰è£…", "Flash Attention æœªå®‰è£…"))
    elif flash_attn_result.get('critical_modules_ok', False):
        summary.append(("Flash Attention", "âœ“ é€šè¿‡", "Flash Attention å…³é”®æ¨¡å—æ­£å¸¸"))
    else:
        summary.append(("Flash Attention", "âœ— å¤±è´¥", "Flash Attention å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥"))
    
    # vLLM
    vllm_result = all_results.get('vllm', {})
    if 'error' in vllm_result:
        summary.append(("vLLM", "é”™è¯¯", "æµ‹è¯•è¿‡ç¨‹å‡ºé”™"))
    elif not vllm_result.get('installed', False):
        summary.append(("vLLM", "âš  æœªå®‰è£…", "vLLM æœªå®‰è£…"))
    elif vllm_result.get('critical_modules_ok', False):
        summary.append(("vLLM", "âœ“ é€šè¿‡", "vLLM å…³é”®æ¨¡å—æ­£å¸¸"))
    else:
        summary.append(("vLLM", "âœ— å¤±è´¥", "vLLM å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥"))
    
    # æ‰“å°æ±‡æ€»è¡¨
    print(f"\n{'æ¨¡å—':<20} {'çŠ¶æ€':<15} {'è¯´æ˜'}")
    print("-" * 80)
    for module, status, desc in summary:
        print(f"{module:<20} {status:<15} {desc}")
    
    # ç»Ÿè®¡
    passed = sum(1 for _, status, _ in summary if 'âœ“' in status)
    failed = sum(1 for _, status, _ in summary if 'âœ—' in status)
    skipped = sum(1 for _, status, _ in summary if 'âš ' in status or 'é”™è¯¯' in status)
    total = len(summary)
    
    print(f"\næ€»è®¡: {total} ä¸ªæµ‹è¯•æ¨¡å—")
    print(f"  é€šè¿‡: {passed}")
    print(f"  å¤±è´¥: {failed}")
    print(f"  è·³è¿‡/é”™è¯¯: {skipped}")
    
    # æœ€ç»ˆç»“è®º
    print_section("æœ€ç»ˆç»“è®º", 1)
    if failed == 0 and skipped == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç¯å¢ƒé…ç½®æ­£å¸¸ã€‚")
        return 0
    elif failed == 0:
        print("âš ï¸  éƒ¨åˆ†æ¨¡å—æœªå®‰è£…æˆ–è·³è¿‡ï¼Œä½†å·²å®‰è£…çš„æ¨¡å—å‡æ­£å¸¸ã€‚")
        return 0
    else:
        print("âŒ å‘ç°ç¯å¢ƒé—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯å¹¶æŒ‰ç…§å»ºè®®è¿›è¡Œä¿®å¤ã€‚")
        return 1

if __name__ == '__main__':
    sys.exit(main())

