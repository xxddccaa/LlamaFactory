#!/usr/bin/env python3
"""
éªŒè¯ Flash Attention æ˜¯å¦æ­£ç¡®å®‰è£…å’Œå¯ç”¨
ç”¨äºæ£€æŸ¥ flash-attn æ˜¯å¦å¯ä»¥æ­£å¸¸å¯¼å…¥å’Œä½¿ç”¨
"""

import sys
import subprocess

def check_flash_attn_installation():
    """æ£€æŸ¥ Flash Attention å®‰è£…"""
    print("=" * 60)
    print("æµ‹è¯• 1: Flash Attention å®‰è£…æ£€æŸ¥")
    print("=" * 60)
    
    try:
        import flash_attn
        version = getattr(flash_attn, '__version__', 'æœªçŸ¥')
        print(f"âœ“ flash_attn ç‰ˆæœ¬: {version}")
        return True, version
    except ImportError as e:
        print(f"âœ— flash_attn æœªå®‰è£…")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        return False, None
    except Exception as e:
        print(f"âœ— flash_attn å¯¼å…¥å¤±è´¥")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        return False, None

def check_pytorch():
    """æ£€æŸ¥ PyTorch ç¯å¢ƒ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: PyTorch ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    try:
        import torch
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"  CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"  GPU æ•°é‡: {torch.cuda.device_count()}")
            
            # è·å– CUDA é©±åŠ¨ç‰ˆæœ¬
            try:
                result = subprocess.run(
                    ['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0 and result.stdout.strip():
                    driver_version = result.stdout.strip().split('\n')[0]
                    print(f"  CUDA é©±åŠ¨ç‰ˆæœ¬: {driver_version}")
            except:
                pass
        
        return True
    except ImportError as e:
        print(f"âœ— PyTorch æœªå®‰è£…: {e}")
        return False
    except Exception as e:
        print(f"âœ— PyTorch æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_flash_attn_critical_modules():
    """æµ‹è¯• Flash Attention å…³é”®æ¨¡å—ï¼ˆè¿™æ˜¯ä¼šå‡ºé”™çš„åœ°æ–¹ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: Flash Attention å…³é”®æ¨¡å—å¯¼å…¥ï¼ˆå…³é”®æµ‹è¯•ï¼‰")
    print("=" * 60)
    
    try:
        # æµ‹è¯• flash_attn ä¸»æ¨¡å—
        import flash_attn
        print("âœ“ flash_attn ä¸»æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯• flash_attn.flash_attn_interfaceï¼ˆC++ æ‰©å±•ï¼Œæœ€å®¹æ˜“å‡ºé”™ï¼‰
        try:
            import flash_attn.flash_attn_interface
            print("âœ“ flash_attn.flash_attn_interface å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âš  flash_attn.flash_attn_interface å¯¼å…¥å¤±è´¥: {e}")
            print("  è¿™å¯èƒ½æ˜¯ç¼–è¯‘é—®é¢˜ï¼Œä½†å¯èƒ½ä¸å½±å“ä½¿ç”¨")
        
        # æµ‹è¯• flash_attn çš„å…³é”®å‡½æ•°
        try:
            from flash_attn import flash_attn_func
            print("âœ“ flash_attn_func å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âœ— flash_attn_func å¯¼å…¥å¤±è´¥: {e}")
            print("\nè¿™æ˜¯å¯¼è‡´è®­ç»ƒå¤±è´¥çš„å…³é”®é”™è¯¯ï¼")
            return False
        
        # æµ‹è¯• flash_attn çš„å…¶ä»–å…³é”®ç»„ä»¶
        try:
            from flash_attn import flash_attn_varlen_func
            print("âœ“ flash_attn_varlen_func å¯¼å…¥æˆåŠŸ")
        except ImportError:
            print("âš  flash_attn_varlen_func ä¸å¯ç”¨ï¼ˆå¯é€‰ï¼‰")
        
        try:
            from flash_attn import flash_attn_with_kvcache
            print("âœ“ flash_attn_with_kvcache å¯¼å…¥æˆåŠŸ")
        except ImportError:
            print("âš  flash_attn_with_kvcache ä¸å¯ç”¨ï¼ˆå¯é€‰ï¼‰")
        
        print("\nâœ“ Flash Attention å…³é”®æ¨¡å—å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
        return True
        
    except ImportError as e:
        print(f"âœ— Flash Attention å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("\nè¿™æ˜¯å¯¼è‡´è®­ç»ƒå¤±è´¥çš„å…³é”®é”™è¯¯ï¼")
        print("\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
        print("1. é‡æ–°å®‰è£… flash-attn:")
        print("   pip uninstall flash-attn -y")
        print("   pip install flash-attn --no-build-isolation")
        print("\n2. æˆ–è€…ä»æºç ç¼–è¯‘ï¼ˆå¦‚æœé¢„ç¼–è¯‘ç‰ˆæœ¬ä¸å…¼å®¹ï¼‰:")
        print("   pip install flash-attn --no-build-isolation")
        print("\n3. æ£€æŸ¥ CUDA å’Œ PyTorch ç‰ˆæœ¬å…¼å®¹æ€§")
        print("   Flash Attention éœ€è¦ CUDA 11.6+ å’Œå…¼å®¹çš„ PyTorch ç‰ˆæœ¬")
        return False
    except Exception as e:
        error_msg = str(e)
        print(f"âœ— Flash Attention å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        
        # è¯Šæ–­å¸¸è§é”™è¯¯
        if 'undefined symbol' in error_msg:
            print("\n" + "-" * 60)
            print("ç¬¦å·é”™è¯¯è¯Šæ–­:")
            print("-" * 60)
            print("æ£€æµ‹åˆ°ç¬¦å·æœªå®šä¹‰é”™è¯¯ï¼Œå¯èƒ½çš„åŸå› ï¼š")
            print("1. Flash Attention ä¸å½“å‰ PyTorch ç‰ˆæœ¬ä¸å…¼å®¹")
            print("2. CUDA åº“è·¯å¾„é…ç½®ä¸æ­£ç¡®")
            print("3. Flash Attention éœ€è¦é‡æ–°ç¼–è¯‘")
            print("\nå»ºè®®ï¼š")
            print("1. è®¾ç½® CUDA åº“è·¯å¾„:")
            print("   export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:$LD_LIBRARY_PATH")
            print("   export LD_PRELOAD=/usr/local/nvidia/lib64/libcuda.so")
            print("\n2. é‡æ–°å®‰è£… flash-attn")
            print("   pip uninstall flash-attn -y")
            print("   pip install flash-attn --no-build-isolation")
        
        return False

def test_flash_attn_functionality():
    """æµ‹è¯• Flash Attention åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: Flash Attention åŸºæœ¬åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        import torch
        from flash_attn import flash_attn_func
        
        if not torch.cuda.is_available():
            print("âš  è·³è¿‡åŠŸèƒ½æµ‹è¯•ï¼šæœªæ£€æµ‹åˆ° GPU")
            return True
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        seq_len = 128
        num_heads = 8
        head_dim = 64
        
        device = 'cuda'
        dtype = torch.float16
        
        q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        
        # æµ‹è¯• flash_attn_func
        try:
            output = flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False)
            print(f"âœ“ flash_attn_func æ‰§è¡ŒæˆåŠŸ")
            print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
            # æµ‹è¯• causal attention
            output_causal = flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=True)
            print(f"âœ“ causal attention æ‰§è¡ŒæˆåŠŸ")
            
            print("âœ“ Flash Attention åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
            return True
        except Exception as e:
            print(f"âœ— flash_attn_func æ‰§è¡Œå¤±è´¥: {e}")
            print("  è¿™å¯èƒ½æ˜¯ CUDA ç¼–è¯‘é—®é¢˜æˆ–ç‰ˆæœ¬ä¸å…¼å®¹")
            return False
        
    except ImportError as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âœ— Flash Attention åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_flash_attn_ops():
    """æµ‹è¯• Flash Attention çš„å…¶ä»–æ“ä½œ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: Flash Attention å…¶ä»–æ“ä½œ")
    print("=" * 60)
    
    try:
        # æµ‹è¯• flash_attn.ops
        try:
            from flash_attn.ops import fused_dense
            print("âœ“ flash_attn.ops.fused_dense å¯ç”¨")
        except ImportError:
            print("âš  flash_attn.ops.fused_dense ä¸å¯ç”¨ï¼ˆå¯é€‰ï¼‰")
        
        try:
            from flash_attn.ops import rms_norm
            print("âœ“ flash_attn.ops.rms_norm å¯ç”¨")
        except ImportError:
            print("âš  flash_attn.ops.rms_norm ä¸å¯ç”¨ï¼ˆå¯é€‰ï¼‰")
        
        try:
            from flash_attn.ops import rotary
            print("âœ“ flash_attn.ops.rotary å¯ç”¨")
        except ImportError:
            print("âš  flash_attn.ops.rotary ä¸å¯ç”¨ï¼ˆå¯é€‰ï¼‰")
        
        return True
    except Exception as e:
        print(f"âš  å…¶ä»–æ“ä½œæ£€æŸ¥å¤±è´¥: {e}")
        return True  # è¿™äº›æ˜¯å¯é€‰çš„ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("Flash Attention å®‰è£…å’Œå¯ç”¨æ€§æµ‹è¯•")
    print("=" * 60 + "\n")
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    flash_attn_installed, flash_attn_version = check_flash_attn_installation()
    if not flash_attn_installed:
        print("\n" + "=" * 60)
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 60)
        print("âœ— Flash Attention æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… flash-attn")
        print("\nå®‰è£…å‘½ä»¤:")
        print("  pip install flash-attn --no-build-isolation")
        print("\næ³¨æ„: Flash Attention éœ€è¦ CUDA 11.6+ å’Œå…¼å®¹çš„ PyTorch ç‰ˆæœ¬")
        return 1
    
    results.append(("Flash Attention å®‰è£…", flash_attn_installed))
    results.append(("PyTorch ç¯å¢ƒ", check_pytorch()))
    results.append(("Flash Attention å…³é”®æ¨¡å—", test_flash_attn_critical_modules()))
    
    # åªåœ¨å…³é”®æ¨¡å—é€šè¿‡æ—¶æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    if results[-1][1]:  # å¦‚æœå…³é”®æ¨¡å—æµ‹è¯•é€šè¿‡
        results.append(("Flash Attention åŸºæœ¬åŠŸèƒ½", test_flash_attn_functionality()))
        results.append(("Flash Attention å…¶ä»–æ“ä½œ", test_flash_attn_ops()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Flash Attention å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
        
        # å¦‚æœå…³é”®æ¨¡å—å¤±è´¥ï¼Œæä¾›å¿«é€Ÿä¿®å¤å»ºè®®
        critical_failed = len(results) >= 3 and not results[2][1]
        if critical_failed:
            print("\n" + "=" * 60)
            print("å¿«é€Ÿä¿®å¤å»ºè®®")
            print("=" * 60)
            print("å¦‚æœ Flash Attention å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå¯ä»¥å°è¯•ï¼š")
            print("\n1. é‡æ–°å®‰è£… flash-attn:")
            print("   pip uninstall flash-attn -y")
            print("   pip install flash-attn --no-build-isolation")
            print("\n2. è®¾ç½® CUDA åº“è·¯å¾„ï¼ˆå¦‚æœé‡åˆ°ç¬¦å·é”™è¯¯ï¼‰:")
            print("   export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:$LD_LIBRARY_PATH")
            print("   export LD_PRELOAD=/usr/local/nvidia/lib64/libcuda.so")
            print("\n3. æ£€æŸ¥ PyTorch å’Œ CUDA ç‰ˆæœ¬å…¼å®¹æ€§")
            print("   Flash Attention éœ€è¦ CUDA 11.6+ å’Œå…¼å®¹çš„ PyTorch ç‰ˆæœ¬")
            print("\n4. å¦‚æœé¢„ç¼–è¯‘ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œå¯èƒ½éœ€è¦ä»æºç ç¼–è¯‘")
        
        return 1

if __name__ == "__main__":
    sys.exit(main())

