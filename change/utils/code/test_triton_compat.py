#!/usr/bin/env python3
"""
æµ‹è¯• PyTorch å’Œ Triton å…¼å®¹æ€§çš„è„šæœ¬
ç”¨äºéªŒè¯ triton_key å¯¼å…¥æ˜¯å¦æ­£å¸¸
"""

import sys
import os
import shutil
import subprocess

def get_cuda_driver_version():
    """è·å– CUDA é©±åŠ¨ç‰ˆæœ¬"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0 and result.stdout.strip():
            return result.stdout.strip().split('\n')[0]
    except:
        pass
    
    try:
        result = subprocess.run(
            ['nvidia-smi'],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            for line in result.stdout.split('\n'):
                if 'Driver Version:' in line:
                    return line.split('Driver Version:')[1].strip().split()[0]
    except:
        pass
    
    return None

def check_cuda_library_paths():
    """æ£€æŸ¥ CUDA åº“è·¯å¾„é…ç½®"""
    print("\n" + "-" * 60)
    print("CUDA åº“è·¯å¾„æ£€æŸ¥")
    print("-" * 60)
    
    # æ£€æŸ¥ LD_LIBRARY_PATH
    ld_path = os.environ.get('LD_LIBRARY_PATH', '')
    print(f"LD_LIBRARY_PATH: {ld_path if ld_path else '(æœªè®¾ç½®)'}")
    
    # æ£€æŸ¥ LD_PRELOAD
    ld_preload = os.environ.get('LD_PRELOAD', '')
    print(f"LD_PRELOAD: {ld_preload if ld_preload else '(æœªè®¾ç½®)'}")
    
    # æ£€æŸ¥å¸¸è§çš„ CUDA åº“è·¯å¾„
    common_paths = [
        '/usr/local/nvidia/lib64',
        '/usr/local/cuda/lib64',
        '/usr/lib/x86_64-linux-gnu',
    ]
    
    libcuda_found = False
    for path in common_paths:
        libcuda_path = os.path.join(path, 'libcuda.so')
        if os.path.exists(libcuda_path):
            print(f"âœ“ æ‰¾åˆ° libcuda.so: {libcuda_path}")
            libcuda_found = True
            if '/usr/local/nvidia/lib64' in path and '/usr/local/nvidia/lib64' not in ld_path:
                print(f"  âš  å»ºè®®: å°† {path} æ·»åŠ åˆ° LD_LIBRARY_PATH")
            break
    
    if not libcuda_found:
        print("âš  æœªæ‰¾åˆ° libcuda.soï¼Œè¿™å¯èƒ½å¯¼è‡´ cuModuleGetFunction é”™è¯¯")
    
    return libcuda_found

def test_imports():
    """æµ‹è¯•åŸºæœ¬å¯¼å…¥"""
    print("=" * 60)
    print("æµ‹è¯• 1: åŸºæœ¬å¯¼å…¥")
    print("=" * 60)
    
    try:
        import torch
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"  GPU æ•°é‡: {torch.cuda.device_count()}")
            
            # è·å– CUDA é©±åŠ¨ç‰ˆæœ¬
            driver_version = get_cuda_driver_version()
            if driver_version:
                print(f"  CUDA é©±åŠ¨ç‰ˆæœ¬: {driver_version}")
    except ImportError as e:
        print(f"âœ— PyTorch å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import triton
        print(f"âœ“ Triton ç‰ˆæœ¬: {triton.__version__}")
    except ImportError as e:
        print(f"âœ— Triton å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥ CUDA åº“è·¯å¾„
    check_cuda_library_paths()
    
    return True

def test_triton_key():
    """æµ‹è¯• triton_key å¯¼å…¥ï¼ˆå…³é”®æµ‹è¯•ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: triton_key å¯¼å…¥ï¼ˆå…³é”®æµ‹è¯•ï¼‰")
    print("=" * 60)
    
    try:
        from triton.compiler.compiler import triton_key
        print("âœ“ triton_key å¯¼å…¥æˆåŠŸï¼")
        print(f"  triton_key ç±»å‹: {type(triton_key)}")
        return True
    except ImportError as e:
        print(f"âœ— triton_key å¯¼å…¥å¤±è´¥: {e}")
        print("\nè¿™æ˜¯å¯¼è‡´è®­ç»ƒå¤±è´¥çš„å…³é”®é”™è¯¯ï¼")
        return False
    except Exception as e:
        print(f"âœ— å…¶ä»–é”™è¯¯: {e}")
        return False

def test_torch_inductor():
    """æµ‹è¯• PyTorch Inductorï¼ˆä½¿ç”¨ triton_key çš„åœ°æ–¹ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: PyTorch Inductor ç¼“å­˜ç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        import torch
        from torch._inductor.codecache import CacheBase
        
        # å°è¯•è·å–ç³»ç»Ÿä¿¡æ¯ï¼ˆè¿™ä¼šè°ƒç”¨ triton_keyï¼‰
        system_info = CacheBase.get_system()
        print("âœ“ PyTorch Inductor ç¼“å­˜ç³»ç»Ÿæ­£å¸¸")
        print(f"  ç³»ç»Ÿä¿¡æ¯é”®: {list(system_info.keys())[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ªé”®
        return True
    except ImportError as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âœ— æ‰§è¡Œå¤±è´¥: {e}")
        print("  è¿™å¯èƒ½æ˜¯ triton_key å¯¼å…¥é—®é¢˜å¯¼è‡´çš„")
        return False

def get_triton_cache_dir():
    """è·å– Triton ç¼“å­˜ç›®å½•"""
    cache_dir = os.environ.get('TRITON_CACHE_DIR')
    if cache_dir:
        return cache_dir
    
    # é»˜è®¤ç¼“å­˜ç›®å½•
    home = os.path.expanduser('~')
    return os.path.join(home, '.triton', 'cache')

def clear_triton_cache():
    """æ¸…é™¤ Triton ç¼“å­˜"""
    cache_dir = get_triton_cache_dir()
    if os.path.exists(cache_dir):
        try:
            shutil.rmtree(cache_dir)
            print(f"âœ“ å·²æ¸…é™¤ Triton ç¼“å­˜: {cache_dir}")
            return True
        except Exception as e:
            print(f"âš  æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False
    else:
        print(f"â„¹ Triton ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}")
        return True

def test_basic_torch_compile():
    """æµ‹è¯•åŸºæœ¬çš„ torch.compile åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: torch.compile åŸºæœ¬åŠŸèƒ½")
    print("=" * 60)
    
    try:
        import torch
        
        @torch.compile
        def simple_add(x, y):
            return x + y
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        x = torch.randn(10, 10, device='cuda' if torch.cuda.is_available() else 'cpu')
        y = torch.randn(10, 10, device='cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ‰§è¡Œï¼ˆç¬¬ä¸€æ¬¡ä¼šè§¦å‘ç¼–è¯‘ï¼‰
        result = simple_add(x, y)
        print("âœ“ torch.compile æµ‹è¯•é€šè¿‡")
        print(f"  ç»“æœå½¢çŠ¶: {result.shape}")
        return True
    except ImportError as e:
        error_msg = str(e)
        if 'undefined symbol' in error_msg or 'cuModuleGetFunction' in error_msg:
            print(f"âœ— torch.compile æµ‹è¯•å¤±è´¥: {e}")
            print("\n" + "=" * 60)
            print("è¯Šæ–­ä¿¡æ¯")
            print("=" * 60)
            print("è¿™ä¸ªé”™è¯¯é€šå¸¸ç”±ä»¥ä¸‹åŸå› å¼•èµ·ï¼š")
            print("1. CUDA é©±åŠ¨åº“è·¯å¾„é…ç½®ä¸æ­£ç¡®ï¼Œæ‰¾ä¸åˆ° libcuda.so")
            print("2. Triton ç¼–è¯‘çš„ä»£ç æ— æ³•é“¾æ¥åˆ°æ­£ç¡®çš„ CUDA Driver API")
            print("3. LD_LIBRARY_PATH æˆ– LD_PRELOAD æœªæ­£ç¡®è®¾ç½®")
            print("\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
            print("1. è®¾ç½® CUDA åº“è·¯å¾„ï¼ˆæ¨èï¼‰ï¼š")
            print("   export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:$LD_LIBRARY_PATH")
            print("   export LD_PRELOAD=/usr/local/nvidia/lib64/libcuda.so")
            print("\n2. æ¸…é™¤ Triton ç¼“å­˜åé‡è¯•ï¼š")
            cache_dir = get_triton_cache_dir()
            print(f"   rm -rf {cache_dir}")
            print("\n3. æ£€æŸ¥ CUDA é©±åŠ¨ç‰ˆæœ¬ï¼š")
            print("   nvidia-smi")
            print("\n4. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯èƒ½éœ€è¦ï¼š")
            print("   - æ£€æŸ¥ /usr/local/nvidia/lib64/libcuda.so æ˜¯å¦å­˜åœ¨")
            print("   - å°è¯•æ›´æ–° Triton ç‰ˆæœ¬ï¼ˆå¦‚ 3.4.0 æˆ– 3.4.1ï¼‰")
            print("   - æ£€æŸ¥ CUDA é©±åŠ¨ç‰ˆæœ¬æ˜¯å¦ä¸ PyTorch å…¼å®¹")
            return False
        else:
            print(f"âœ— torch.compile æµ‹è¯•å¤±è´¥: {e}")
            return False
    except Exception as e:
        print(f"âœ— torch.compile æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("PyTorch å’Œ Triton å…¼å®¹æ€§æµ‹è¯•")
    print("=" * 60 + "\n")
    
    # æ˜¾ç¤º Triton ç¼“å­˜ä¿¡æ¯
    cache_dir = get_triton_cache_dir()
    print(f"Triton ç¼“å­˜ç›®å½•: {cache_dir}")
    if os.path.exists(cache_dir):
        cache_size = sum(
            os.path.getsize(os.path.join(dirpath, filename))
            for dirpath, dirnames, filenames in os.walk(cache_dir)
            for filename in filenames
        )
        print(f"  ç¼“å­˜å¤§å°: {cache_size / (1024*1024):.2f} MB")
    print()
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("åŸºæœ¬å¯¼å…¥", test_imports()))
    results.append(("triton_key å¯¼å…¥", test_triton_key()))
    results.append(("PyTorch Inductor", test_torch_inductor()))
    
    # åªåœ¨æœ‰ GPU æ—¶æµ‹è¯• compile
    compile_failed = False
    try:
        import torch
        if torch.cuda.is_available():
            compile_result = test_basic_torch_compile()
            results.append(("torch.compile", compile_result))
            if not compile_result:
                compile_failed = True
        else:
            print("\n" + "=" * 60)
            print("æµ‹è¯• 4: torch.compile åŸºæœ¬åŠŸèƒ½")
            print("=" * 60)
            print("âš  è·³è¿‡ï¼šæœªæ£€æµ‹åˆ° GPU")
    except:
        pass
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼PyTorch å’Œ Triton å…¼å®¹æ€§æ­£å¸¸ã€‚")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
        
        # å¦‚æœ compile å¤±è´¥ï¼Œæä¾›å¿«é€Ÿä¿®å¤å»ºè®®
        if compile_failed:
            print("\n" + "=" * 60)
            print("å¿«é€Ÿä¿®å¤å»ºè®®")
            print("=" * 60)
            print("å¦‚æœ torch.compile å¤±è´¥ï¼ˆcuModuleGetFunction é”™è¯¯ï¼‰ï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
            print("\næ­¥éª¤ 1: è®¾ç½® CUDA åº“è·¯å¾„")
            print("  export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:$LD_LIBRARY_PATH")
            print("  export LD_PRELOAD=/usr/local/nvidia/lib64/libcuda.so")
            print("\næ­¥éª¤ 2: æ¸…é™¤ Triton ç¼“å­˜")
            print(f"  rm -rf {cache_dir}")
            print("\næ­¥éª¤ 3: é‡æ–°è¿è¡Œæµ‹è¯•")
            print("  python '/mnt/s3fs/swifttrain/utils/code/test_triton_compat.py'")
            print("\næˆ–è€…ä¸€é”®æ‰§è¡Œï¼š")
            print("  export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:$LD_LIBRARY_PATH && \\")
            print("  export LD_PRELOAD=/usr/local/nvidia/lib64/libcuda.so && \\")
            print(f"  rm -rf {cache_dir} && \\")
            print("  python '/mnt/s3fs/swifttrain/utils/code/test_triton_compat.py'")
        
        return 1

if __name__ == "__main__":
    sys.exit(main())

