model_name_or_path: /data/xiedong/Qwen3-VL-2B-Instruct/
image_max_pixels: 262144
video_max_pixels: 16384

do_train: true
stage: sft
finetuning_type: full
dataset: llava_1k_en  # 或使用 mllm_demo
template: qwen3_vl_nothink
cutoff_len: 4096

output_dir: saves/mca/qwen3_vl_test
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
num_train_epochs: 2
learning_rate: 3e-6
bf16: true

# Megatron 并行配置
# 注意：pipeline_model_parallel_size * tensor_model_parallel_size 必须 <= 可用 GPU 数量
# 如果只有 1 个 GPU，两个参数都应该设置为 1
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 4  # 从 4 改为 1，因为只有 1 个 GPU
sequence_parallel: false
bias_activation_fusion: true
apply_rope_fusion: true
use_distributed_optimizer: true